---
title: "Bruce Changlong Xu"
description: "AI, healthcare, and the road to superintelligence."
---

I currently support frontier AI training and inference systems at _Cerebras_, where we work with the world's most pioneering enterprises, governments, and institutions such as GlaxoSmithKline, Novo Nordisk, Perplexity, Mistral, G42 and other leaders to build transformative artificial intelligence.  

I have the privilege to support my talented friends at [3Cap AGI Partners](https://3cagi.vc/) to invest in the next generation of companies defining AGI infrastructure. Alongside them, I was fortunate to personally invest in [Synchron's Series D](https://synchron.com/), which is pioneering endovascular brain-computer interface technology for patients worldwide. As a member of _Asia Society's Dragon and Eagle Rising Leaders Council_, I am part of a wonderful community that helps to promote crucial dialogue between the East and the West in areas such as AI Governance and Oncology Clinical Trial Harmonization. 

I have previously worked at _Apple_ and _NVIDIA_, and completed my undergraduate and graduate studies in Mathematics and Computer Science at _Stanford University_, working with phenomenal researchers at [SAIL](https://ai.stanford.edu/), [HAI](https://hai.stanford.edu/), [Stanford Cardiovascular Institute](https://www.ahajournals.org/doi/10.1161/CIRCRESAHA.124.325652), and [Stanford Neurosurgery](https://ieeexplore.ieee.org/document/11058715). Earlier in my life, I was deeply involved Olympiad Mathematics, earning Gold/Silver medals at the British and Asian Pacific Mathematical Olympiads, and representing Hong Kong at the International Mathematical Olympiad as HKG01.

Together with cofounders, I started _Meirona_ a stealth platform building a global pervasive fabric defining multimodal AI infrastructure for science and medicine. Outside of work, I stay grounded through endurance running, music, and time with my border collie Aegis, and of course family. 

---

### A Hitchhiker's Timeline of Modern-day AGI

Modern AI rides on more than 70 years of compounding technologies. From the 1947 transistor and 1958 integrated circuit to the 1971 microprocessor and deep-submicron CMOS. This foundation led to the invention fo the modern day AI processor, and in 2006 CUDA made GPUs generally programmable - in 2010-1015, Fermi, Kepler and Maxwell normalized massive parallelism and device memory hierarchies within and across GPUs, to train the intelligence we now call AI. The art of intimate "co-design" between hardware and software evolved to make sure that AI was being fed with meaningful work and data - in 2017 the "Tensor Core" (and corresponding software advances such as CUTLASS) was introduced that revolutionized the possibility of GEMMs (general matrix multiplies) at global economics scale, alongside innovations such as the TPU, Cerebras Wafer and more. Transformers became important enough that entire cores were introduced to accelerate this particular workload (H100s introduced "Transformer Engines" in FP8 to truly optimize attention and MLP). cuDNN, NCCL, Horovod industrialized multi-GPU training; leading to innovations such as Megatron, DeepSpeed that enabled tensor, pipeline, data parallel training paradigms to complement sharding (ZeRO/FSDP) techniques, activation checkpointing and optimizer state partitioning - trillion parameter models could perceivably be accomplished even in light of the memory wall. Every processor could be individually tightly controlled with JAX, Triton and custom-kernels (e.g. FlashAttention), and orchestrated alongside its peers with these new techniques. 

Once compute became abundant, models proliferated in what some called a "cambrian explosion". In 2012, Krizhevsky et al invented AlexNet that used ReLU, dropout and fine-grained control over GPUs to catalyze the initial sparks of representation learning at scale; Then in 2015, He Kaiming and his team's ResNet demonstrated that we could truly train "deep" neural networks through the novel idea of skip connections that allowed us to bypass and communicate across longer ranges between layers, without information decay. In 2016, AlphaGo demonstrated the power of reinforcement learning. In 2017, recurrent neural networks matured to the modern day transformer, which set fire to modern day pre-training, post-training and infrastructure paradigms governed by beautiful scaling laws - delivering BERT in 2018, GPT-2/3 in 2019/20 and multimodal models that could deeply connect representations of images, text, voice and indeed the perceivable universe of digital data. CLIP fusioned vision and language through a contrastive learning paradigm for zero-shot transfer, and we began to create a wave of "generative AI" applications that could design new proteins for human health through the exact same diffusion mechanism that created imaginative text to image tools. With DeepSeek's R1 paper, RLHF, SFT and PPO across mixtures of experts began to take center stage, and the community began to think deeply about to train and serve models more efficiently, and increase the "density" of intelligence per token/weight. vLLM, QLoRA, GPTQ and speculative decoding used student-teacher, distillation, quantization and paged-caching paradigms to deliver world-class performance at scale and new alignment paradigms such as DPO and RLAIF emerged; alongside a booming interest in "agentic" and autonomous systems with multi-step reasoning and action. 

The same scaling narrative is now playing out in _world models_ - systems that learn latent dynamics so they can _predict_, _plan_, and _act_ in the real physical world. How do we create policies, and use spatiotemporal encoders (with Transformer backbones) to reason over compact latent spaces, and close the Sim2Real gap? LeCunn's JEPA paradigm and new data-hungry Vision-Language-Action structures are beginning to take form to truly, deeply and meaningfully embed and embody advanced machine intelligence in the real world. 

Threaded through all of these advancements is the deeply _"human layer"_ that turns capability into dependable tools, through domain-specific post-training the encodes policy, ontology, and constraints. It is paramount to work towards AGI that incrementally widens what is routine; models that read, cite, plan and excute; agents that coorperate under governance; AI training, evaluation and deployment stacks that do not collapse under the auspice of low-quality and potentially adversarial data. From science and medicine to technology and education, codified intelligence is now reshaping and redefining human flourishing. Thanks for reading this far, and welcome to my blog. 

