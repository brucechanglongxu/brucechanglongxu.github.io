---
title: "Bruce Changlong Xu"
description: "AI, healthcare, and the road to superintelligence."
---

I currently support frontier AI training and inference systems at **Cerebras**, where we work with the world's most pioneering enterprises, governments, and institutions such as GlaxoSmithKline, Novo Nordisk, Perplexity, Mistral, G42 and other leaders to build transformative artificial intelligence.  

I have the privilege to support my talented friends at [3Cap AGI Partners](https://3cagi.vc/) to invest in the next generation of companies defining AGI infrastructure. Alongside them, I was fortunate to personally invest in [Synchron's Series D](https://synchron.com/), which is pioneering endovascular brain-computer interface technology for patients worldwide. As a member of _Asia Society's Dragon and Eagle Rising Leaders Council_, I am part of a wonderful community that helps to promote crucial dialogue between the East and the West in areas such as AI Governance and Oncology Clinical Trial Harmonization. 

I have previously worked at **Apple** and **NVIDIA**, and completed my undergraduate and graduate studies in Mathematics and Computer Science at **Stanford University**, working with phenomenal researchers at [SAIL](https://ai.stanford.edu/), [HAI](https://hai.stanford.edu/), [Stanford Cardiovascular Institute](https://www.ahajournals.org/doi/10.1161/CIRCRESAHA.124.325652), and [Stanford Neurosurgery](https://ieeexplore.ieee.org/document/11058715). Earlier in my life, I was deeply involved Olympiad Mathematics, earning Gold/Silver medals at the British and Asian Pacific Mathematical Olympiads, and representing Hong Kong at the International Mathematical Olympiad as HKG01.

Together with cofounders, I started _Meirona_ a stealth platform building a global pervasive fabric defining multimodal AI infrastructure for science and medicine. Outside of work, I stay grounded through endurance running, music, and time with my border collie Aegis, and of course family. 

---

Modern AI rides on more than 70 years of compounding technologies. From the 1947 transistor and 1958 integrated circuit to the 1971 microprocessor and deep-submicron CMOS. This foundation led to the invention fo the modern day AI processor, and in 2006 CUDA made GPUs generally programmable - in 2010-1015, Fermi, Kepler and Maxwell normalized massive parallelism and device memory hierarchies within and across GPUs, to train the intelligence we now call AI. The art of intimate "co-design" between hardware and software evolved to make sure that AI was being fed with meaningful work and data - in 2017 the "Tensor Core" (and corresponding software advances such as CUTLASS) was introduced that revolutionized the possibility of GEMMs (general matrix multiplies) at global economics scale, alongside innovations such as the TPU, Cerebras Wafer and more. cuDNN, NCCL, Horovod industrialized multi-GPU training; leading to innovations such as Megatron, DeepSpeed that enabled tensor, pipeline, data parallel training paradigms to complement sharding (ZeRO/FSDP) techniques, activation checkpointing and optimizer state partitioning - trillion parameter models could perceivably be accomplished even in light of the memory wall. Every processor could be individually tightly controlled with JAX, Triton and custom-kernels, and orchestrated alongside its peers with these new techniques. 

Once compute became abundant, models proliferated in what some called a "cambrian explosion". In 2012, Krizhevsky et al invented AlexNet that used ReLU, dropout and fine-grained control over GPUs to catalyze the initial sparks of representation learning at scale; Then in 2015, He Kaiming and his team's ResNet demonstrated that we could truly train "deep" neural networks through the novel idea of skip connections that allowed us to bypass and communicate across longer ranges between layers, without information decay. In 2016, AlphaGo demonstrated the power of reinforcement learning. In 2017, recurrent neural networks matured to the modern day transformer, which set fire to modern day pre-training, post-training and infrastructure paradigms governed by beautiful scaling laws - delivering BERT in 2018, GPT-2/3 in 2019/20 and multimodal models that could deeply connect representations of images, text, voice and indeed the perceivable universe of digital data. CLIP fusioned vision and language through a contrastive learning paradigm for zero-shot transfer, 

Threaded through all of these advancements is the deeply _"human layer"_ that turns capability into dependable tools, through domain-specific post-training the encodes policy, ontology, and constraints. It is paramount to work towards AGI that incrementally widens what is routine; models that read, cite, plan and excute; agents that coorperate under governance; AI training, evaluation and deployment stacks that do not collapse under the auspice of low-quality and potentially adversarial data. 

_Thank you to Dario Amodei for providing the template for this website_
